{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Movie_Review classification using NLP with Naive Bayes</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant modules for use in code\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read multiple review text files from each 'neg' and 'pos' directories using glob module. Each text file \n",
    "# contains a single review for a movie. There are in total 1000 negative reviews and 1000 positive reviews \n",
    "# stored in their respective directories. \n",
    "\n",
    "import glob\n",
    "import errno\n",
    "path_neg = 'C:\\\\Users\\\\Hemang\\\\Desktop\\\\Data Science\\\\Python files\\\\txt_sentoken\\\\neg\\\\*.txt'\n",
    "path_pos = 'C:\\\\Users\\\\Hemang\\\\Desktop\\\\Data Science\\\\Python files\\\\txt_sentoken\\\\pos\\\\*.txt'\n",
    "files_neg = glob.glob(path_neg)\n",
    "files_pos = glob.glob(path_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of all 1000 negative reviews\n",
    "\n",
    "list_neg =[]\n",
    "for file in files_neg:\n",
    "    try:\n",
    "        with open(file,'r') as f:\n",
    "            lst = f.read()\n",
    "            list_neg.append(lst)\n",
    "    except IOError as exc:\n",
    "        if exc.errno != errno.EISDIR:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of all 1000 positive reviews\n",
    "list_pos = []\n",
    "for file in files_pos:\n",
    "    try:\n",
    "        with open(file,'r') as f:\n",
    "            lst = f.read()\n",
    "            list_pos.append(lst)\n",
    "    except IOError as exc:\n",
    "        if exc.errno != errno.EISDIR:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_neg),len(list_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hemang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading stopwords package from nltk module\n",
    "# Creating a list of stop words and punctuations to omit from the 2 lists created above using \n",
    "# bag_of_words_features_filtered() function defined in next cell\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "useless_words = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation)\n",
    "useless_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_features_filtered(words):\n",
    "    return {word:1 for word in words if not word in useless_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hemang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\") # Download word tokenizer 'punkt' from nltk\n",
    "stemmer = PorterStemmer()\n",
    "def clean(lst,sign):\n",
    "    word_list=[]\n",
    "    for i in lst:\n",
    "        word = word_tokenize(i) # split text to each word\n",
    "        word = [stemmer.stem(word) for word in word] # stemming of data\n",
    "        word = [word for word in word if len(word)>2] #removing words less than 3 charters longas they do not contribute much\n",
    "        word = (bag_of_words_features_filtered(word),sign) # returns a tuple with a review and its sign(neg or pos review)\n",
    "        word_list.append(word)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'plot': 1, 'two': 1, 'teen': 1, 'coupl': 1, 'church': 1, 'parti': 1, 'drink': 1, 'drive': 1, 'get': 1, 'accid': 1, 'one': 1, 'guy': 1, 'die': 1, 'girlfriend': 1, 'continu': 1, 'see': 1, 'life': 1, 'nightmar': 1, 'deal': 1, 'watch': 1, 'movi': 1, 'sorta': 1, 'find': 1, 'critiqu': 1, 'mind-fuck': 1, 'gener': 1, 'touch': 1, 'veri': 1, 'cool': 1, 'idea': 1, 'present': 1, 'bad': 1, 'packag': 1, 'make': 1, 'thi': 1, 'review': 1, 'even': 1, 'harder': 1, 'write': 1, 'sinc': 1, 'applaud': 1, 'film': 1, 'attempt': 1, 'break': 1, 'mold': 1, 'mess': 1, 'head': 1, 'lost': 1, 'highway': 1, 'memento': 1, 'good': 1, 'way': 1, 'type': 1, 'folk': 1, \"n't\": 1, 'snag': 1, 'correctli': 1, 'seem': 1, 'taken': 1, 'pretti': 1, 'neat': 1, 'concept': 1, 'execut': 1, 'terribl': 1, 'problem': 1, 'well': 1, 'main': 1, 'simpli': 1, 'jumbl': 1, 'start': 1, 'normal': 1, 'downshift': 1, 'fantasi': 1, 'world': 1, 'audienc': 1, 'member': 1, 'dream': 1, 'charact': 1, 'come': 1, 'back': 1, 'dead': 1, 'look': 1, 'like': 1, 'strang': 1, 'apparit': 1, 'disappear': 1, 'looooot': 1, 'chase': 1, 'scene': 1, 'ton': 1, 'weird': 1, 'thing': 1, 'happen': 1, 'explain': 1, 'person': 1, 'mind': 1, 'tri': 1, 'unravel': 1, 'everi': 1, 'doe': 1, 'give': 1, 'clue': 1, 'kind': 1, 'fed': 1, 'biggest': 1, 'obvious': 1, 'got': 1, 'big': 1, 'secret': 1, 'hide': 1, 'want': 1, 'complet': 1, 'final': 1, 'five': 1, 'minut': 1, 'entertain': 1, 'thrill': 1, 'engag': 1, 'meantim': 1, 'realli': 1, 'sad': 1, 'part': 1, 'arrow': 1, 'dig': 1, 'flick': 1, 'actual': 1, 'figur': 1, 'half-way': 1, 'point': 1, 'littl': 1, 'bit': 1, 'sens': 1, 'still': 1, 'guess': 1, 'bottom': 1, 'line': 1, 'alway': 1, 'sure': 1, 'befor': 1, 'given': 1, 'password': 1, 'enter': 1, 'understand': 1, 'mean': 1, 'show': 1, 'melissa': 1, 'sagemil': 1, 'run': 1, 'away': 1, 'vision': 1, 'throughout': 1, 'plain': 1, 'lazi': 1, 'okay': 1, 'peopl': 1, 'know': 1, 'need': 1, 'differ': 1, 'offer': 1, 'insight': 1, 'appar': 1, 'studio': 1, 'took': 1, 'director': 1, 'chop': 1, 'themselv': 1, 'might': 1, \"'ve\": 1, 'decent': 1, 'somewher': 1, 'suit': 1, 'decid': 1, 'turn': 1, 'music': 1, 'video': 1, 'edg': 1, 'would': 1, 'actor': 1, 'although': 1, 'bentley': 1, 'play': 1, 'exact': 1, 'american': 1, 'beauti': 1, 'onli': 1, 'new': 1, 'neighborhood': 1, 'kudo': 1, 'hold': 1, 'entir': 1, 'feel': 1, 'overal': 1, 'stick': 1, 'becaus': 1, 'confus': 1, 'rare': 1, 'excit': 1, 'redund': 1, 'runtim': 1, 'despit': 1, 'end': 1, 'explan': 1, 'crazi': 1, 'came': 1, 'horror': 1, 'slasher': 1, 'someon': 1, 'assum': 1, 'genr': 1, 'hot': 1, 'kid': 1, 'also': 1, 'wrap': 1, 'product': 1, 'year': 1, 'ago': 1, 'sit': 1, 'shelv': 1, 'ever': 1, 'whatev': 1, 'skip': 1, 'joblo': 1, 'elm': 1, 'street': 1, '7/10': 1, 'blair': 1, 'witch': 1, 'crow': 1, '9/10': 1, 'salvat': 1, '4/10': 1, '10/10': 1, 'stir': 1, 'echo': 1, '8/10': 1}, 'neg')]\n"
     ]
    }
   ],
   "source": [
    "negative_features = clean(list_neg,'neg') # Calling clean function to perform data cleaning on positive review list\n",
    "print(negative_features[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'film': 1, 'adapt': 1, 'comic': 1, 'book': 1, 'plenti': 1, 'success': 1, 'whether': 1, \"'re\": 1, 'superhero': 1, 'batman': 1, 'superman': 1, 'spawn': 1, 'gear': 1, 'toward': 1, 'kid': 1, 'casper': 1, 'arthous': 1, 'crowd': 1, 'ghost': 1, 'world': 1, 'never': 1, 'realli': 1, 'like': 1, 'hell': 1, 'befor': 1, 'starter': 1, 'creat': 1, 'alan': 1, 'moor': 1, 'eddi': 1, 'campbel': 1, 'brought': 1, 'medium': 1, 'whole': 1, 'new': 1, 'level': 1, 'mid': 1, \"'80\": 1, '12-part': 1, 'seri': 1, 'call': 1, 'watchmen': 1, 'say': 1, 'thoroughli': 1, 'research': 1, 'subject': 1, 'jack': 1, 'ripper': 1, 'would': 1, 'michael': 1, 'jackson': 1, 'start': 1, 'look': 1, 'littl': 1, 'odd': 1, 'graphic': 1, 'novel': 1, '500': 1, 'page': 1, 'long': 1, 'includ': 1, 'nearli': 1, 'consist': 1, 'noth': 1, 'footnot': 1, 'word': 1, \"n't\": 1, 'dismiss': 1, 'thi': 1, 'becaus': 1, 'sourc': 1, 'get': 1, 'past': 1, 'thing': 1, 'might': 1, 'find': 1, 'anoth': 1, 'stumbl': 1, 'block': 1, 'director': 1, 'albert': 1, 'allen': 1, 'hugh': 1, 'brother': 1, 'direct': 1, 'seem': 1, 'almost': 1, 'ludicr': 1, 'cast': 1, 'carrot': 1, 'top': 1, 'well': 1, 'anyth': 1, 'riddl': 1, 'better': 1, 'set': 1, 'ghetto': 1, 'featur': 1, 'violent': 1, 'street': 1, 'crime': 1, 'mad': 1, 'genius': 1, 'behind': 1, 'menac': 1, 'societi': 1, 'question': 1, 'cours': 1, 'whitechapel': 1, '1888': 1, 'london': 1, 'east': 1, 'end': 1, 'filthi': 1, 'sooti': 1, 'place': 1, 'whore': 1, 'unfortun': 1, 'nervou': 1, 'mysteri': 1, 'psychopath': 1, 'carv': 1, 'profess': 1, 'surgic': 1, 'precis': 1, 'first': 1, 'stiff': 1, 'turn': 1, 'copper': 1, 'peter': 1, 'godley': 1, 'robbi': 1, 'coltran': 1, 'enough': 1, 'inspector': 1, 'frederick': 1, 'abberlin': 1, 'johnni': 1, 'depp': 1, 'blow': 1, 'crack': 1, 'case': 1, 'widow': 1, 'prophet': 1, 'dream': 1, 'unsuccess': 1, 'tri': 1, 'quell': 1, 'copiou': 1, 'amount': 1, 'absinth': 1, 'opium': 1, 'upon': 1, 'arriv': 1, 'befriend': 1, 'name': 1, 'mari': 1, 'kelli': 1, 'heather': 1, 'graham': 1, 'proce': 1, 'investig': 1, 'horribl': 1, 'gruesom': 1, 'even': 1, 'polic': 1, 'surgeon': 1, 'stomach': 1, 'think': 1, 'anyon': 1, 'need': 1, 'brief': 1, 'particular': 1, 'uniqu': 1, 'interest': 1, 'theori': 1, 'ident': 1, 'killer': 1, 'reason': 1, 'choos': 1, 'slay': 1, 'bother': 1, 'cloak': 1, 'screenwrit': 1, 'terri': 1, 'hay': 1, 'vertic': 1, 'limit': 1, 'rafael': 1, 'yglesia': 1, 'rabl': 1, 'good': 1, 'job': 1, 'keep': 1, 'hidden': 1, 'viewer': 1, 'veri': 1, 'funni': 1, 'watch': 1, 'local': 1, 'blindli': 1, 'point': 1, 'finger': 1, 'blame': 1, 'jew': 1, 'indian': 1, 'englishman': 1, 'could': 1, 'capabl': 1, 'commit': 1, 'ghastli': 1, 'act': 1, 'whistl': 1, 'stonecutt': 1, 'song': 1, 'simpson': 1, 'day': 1, 'hold': 1, 'back': 1, 'electr': 1, 'car/who': 1, 'made': 1, 'steve': 1, 'guttenberg': 1, 'star': 1, 'worri': 1, \"'ll\": 1, 'make': 1, 'sens': 1, 'see': 1, 'onto': 1, 'appear': 1, 'certainli': 1, 'dark': 1, 'bleak': 1, 'surpris': 1, 'much': 1, 'tim': 1, 'burton': 1, 'planet': 1, 'ape': 1, 'time': 1, 'sleepi': 1, 'hollow': 1, 'print': 1, 'saw': 1, 'complet': 1, 'finish': 1, 'color': 1, 'music': 1, 'final': 1, 'comment': 1, 'marilyn': 1, 'manson': 1, 'cinematograph': 1, 'deme': 1, 'abli': 1, 'captur': 1, 'dreari': 1, 'victorian-era': 1, 'help': 1, 'flashi': 1, 'kill': 1, 'scene': 1, 'remind': 1, 'crazi': 1, 'flashback': 1, 'twin': 1, 'peak': 1, 'though': 1, 'violenc': 1, 'pale': 1, 'comparison': 1, 'black-and-whit': 1, 'oscar': 1, 'winner': 1, 'martin': 1, 'child': 1, 'shakespear': 1, 'love': 1, 'product': 1, 'design': 1, 'origin': 1, 'pragu': 1, 'surround': 1, 'one': 1, 'creepi': 1, 'solid': 1, 'dreami': 1, 'typic': 1, 'strong': 1, 'perform': 1, 'deftli': 1, 'handl': 1, 'british': 1, 'accent': 1, 'ian': 1, 'holm': 1, 'joe': 1, 'gould': 1, 'secret': 1, 'richardson': 1, '102': 1, 'dalmatian': 1, 'log': 1, 'great': 1, 'support': 1, 'role': 1, 'big': 1, 'cring': 1, 'open': 1, 'mouth': 1, 'imagin': 1, 'attempt': 1, 'irish': 1, 'actual': 1, 'half': 1, 'bad': 1, 'howev': 1, 'violence/gor': 1, 'sexual': 1, 'languag': 1, 'drug': 1, 'content': 1}, 'pos')]\n"
     ]
    }
   ],
   "source": [
    "positive_features = clean(list_pos,'pos') # Calling clean function to perform data cleaning on negative review list\n",
    "print(positive_features[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 split for training data/test data\n",
    "split = 800  \n",
    "\n",
    "# Fitting training data using Naive Bayes Classifier\n",
    "sentiment_classifier = NaiveBayesClassifier.train(positive_features[:split]+negative_features[:split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy using Training data: 96.0%\n"
     ]
    }
   ],
   "source": [
    "# Find the accuracy, using the training data\n",
    "\n",
    "accuracy_traindata = nltk.classify.util.accuracy(sentiment_classifier,positive_features[:split]+negative_features[:split])*100\n",
    "print('Model Accuracy using Training data: '+str(accuracy_traindata)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy using Training data: 70.25%\n"
     ]
    }
   ],
   "source": [
    "# Find the accuracy, using the test data\n",
    "\n",
    "accuracy_testdata = nltk.classify.util.accuracy(sentiment_classifier,positive_features[split:]+negative_features[split:])*100\n",
    "print('Model Accuracy using Training data: '+str(accuracy_testdata)+'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
